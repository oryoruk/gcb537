#!usr/bin/python

# NAME:  ONUR YORUK
# CLASS: GCB537 - SPRING 2016
# EXERCISE 2 PROGRAMMING

import pickle
import numpy as np
from scipy import linalg
import matplotlib.pyplot as plt


INPUT_DIR = '../Ex2Prog/'
OUTPUT_DIR = '../output/'


# function for residual sum of squares
def rss(y, y_hat):
    return np.power(y - y_hat, 2).sum()

#function that returns indices for cross validation
def CV_fold_indices(total_no, CV_fold_no):
    fold_size = total_no/int(CV_fold_no)
    indices = []
    starts = range(0,total_no,fold_size)[:CV_fold_no]
    ends = range(0, total_no,fold_size)[1:CV_fold_no] + [total_no]
    return zip(starts, ends)

# provided ridge regression function
def ridge(A, b, alphas):
    """
    Return coefficients for regularized least squares

         min ||A x - b||^2 + alpha ||x||^2

    Parameters
    ----------
    A : array, shape (n, p)
    b : array, shape (n,)
    alphas : array, shape (k,)

    Returns
    ----------
    coef: array, shape (p, k)
    """

    U, s, Vt = linalg.svd(A, full_matrices=False)
    d = s / (s[:, np.newaxis].T ** 2 + alphas[:, np.newaxis])
    return np.dot(d * U.T.dot(b), Vt).T


# loading serialized files generated by 'generateX.py'
X_filename = OUTPUT_DIR + 'probs_of_being_regulated.pickle'
y_array_filename = OUTPUT_DIR + 'experiments.pickle'

fileObject = open(X_filename, 'r')
X = pickle.load(fileObject)
fileObject = open(y_array_filename, 'r')
y_array = pickle.load(fileObject)

print y_array
print 'variables loaded'


n, p = X.shape
_, exp_no = y_array.shape
#code to train with cross validation
#set lambdas
k = 200
lambdas = np.logspace(-10,2,k)
cv_fold_no = 10

#for each experiment
for i, exp in enumerate(exp_list):
    #get the y vector that corresponds to the experiment
    y = y_array[:,i]
    test_rss_per_lambda = np.zeros(k,)
    y_hat_per_lambda = np.zeros((n,k))
    #for each fold
    #code to train with cross validation:
    for fold_start,fold_end in CV_fold_indices(n, cv_fold_no):
        fold_train_X = np.concatenate( [X[:fold_start,:],X[fold_end:,:]])
        fold_test_X = X[fold_start:fold_end,:]
        fold_train_y = np.concatenate([y[:fold_start],y[fold_end:]])
        fold_test_y = y[fold_start:fold_end]
        #run ridge regression on the cv training set (fold left out)
        coefs_array = ridge(fold_train_X,fold_train_y,lambdas)
        #for each lambda
        for j in range(k):
            bias_term = (fold_train_y - np.dot(fold_train_X, coefs_array[:,j])).mean()
            fold_test_y_hat = np.dot(fold_test_X, coefs_array[:,j])+bias_term
            #print bias_term, rss(fold_test_y, fold_test_y_hat)
            test_rss_per_lambda[j] += rss(fold_test_y, fold_test_y_hat)
            y_hat_per_lambda[fold_start:fold_end,j] = fold_test_y_hat
    #pick best lambda based on rss
    best_lambda_index = argmin(test_rss_per_lambda)
    best_lambda = lambdas[best_lambda_index]
    print best_lambda
    best_rss, best_biasterm, best_coeffs = 0.0,0.0, []
    #associated rss
    #min(rss_per_lambda)
    #now train using whole data:
    training_rss_per_lambda = np.zeros(k,)
    #run ridge regression on the cv training set (fold left out)
    #running ridge for the entire dataset
    coefs_array = ridge(X,y,lambdas)
    #for each lambda
    for j in range(k):
        bias_term = (y - np.dot(X, coefs_array[:,j])).mean()
        y_hat = np.dot(X, coefs_array[:,j])+bias_term
        #print bias_term, rss(fold_test_y, fold_test_y_hat)
        training_rss_per_lambda[j] += rss(y, y_hat)
        if j == best_lambda_index:
            best_biasterm = bias_term
            best_rss = rss(y, y_hat)
            best_coeffs = coefs_array[:,j]
    #save output file:
    save_output('Expression'+str(i+1)+'.tab', best_lambda, best_rss, best_biasterm, best_coeffs)
    #here comes visualization:
    #code to visualize the relationship between lambda and accuracy (RSS) in test and whole data

    #TODO: visualize training vs all data rss's / lambda
    #TODO: visualize all data complexity / lambda

